{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nltk.util import pad_sequence, bigrams, ngrams, everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends, flatten, padded_everygram_pipeline\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm import MLE, KneserNeyInterpolated\n",
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import nltk\n",
    "from sklearn.neural_network import BernoulliRBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current directories\n",
    "directory = os.getcwd()\n",
    "\n",
    "# Get all the files \n",
    "filepath = os.path.join(directory, 'Gutenberg/txt/')\n",
    "files = os.listdir(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dict for authors and titles\n",
    "titles = dict()\n",
    "text_files = dict()\n",
    "\n",
    "# Check every file\n",
    "for file in files:\n",
    "    # Split the author and title\n",
    "    split = file.split('___')\n",
    "    try:\n",
    "        author = split[0]\n",
    "        title = split[1].split('.')[0]\n",
    "    except:\n",
    "        # Not a valid title file\n",
    "        pass\n",
    "    if author not in titles:\n",
    "        titles[author] = []\n",
    "        text_files[author] = []\n",
    "        \n",
    "    text_files[author].append(file)\n",
    "    titles[author].append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the books from an author\n",
    "# Each book is considered a document now. \n",
    "documents = []\n",
    "books = text_files['Nathaniel Hawthorne']\n",
    "for book in books[0:3]:\n",
    "    file = os.path.join(filepath, book)\n",
    "    with open(file) as f:\n",
    "        data = f.read()\n",
    "        data = data.replace(\"[^a-zA-Z#]\", \"\")\n",
    "        data = data.lower()\n",
    "        documents.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences (tokenize)\n",
    "# This is similar to nltk.sent_tokenize(data) ~I think...\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# sentences = tokenizer.tokenize(data)\n",
    "\n",
    "tokenized = []\n",
    "from gensim.utils import simple_preprocess\n",
    "for doc in documents:\n",
    "    tokenized.append(simple_preprocess(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri-gram (3 words)\n",
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tri-gram model\n",
    "# Use MLE\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_sents)\n",
    "len(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing out some testing result\n",
    "\n",
    "# print(model.vocab.lookup(tokenized[0]))\n",
    "# model.counts['woman'] # It can count there is 43 instances of 'woman'\n",
    "# model.counts[['a']]['woman'] # a woman is 19 instances (Count woman | a)\n",
    "model.vocab.lookup(tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"... build\")\n",
    "brown = nltk.corpus.brown\n",
    "corpus = [word.lower() for word in brown.words()]\n",
    "\n",
    "# Train on 95% f the corpus and test on the rest\n",
    "spl = round(95*len(corpus)/100)\n",
    "print(spl)\n",
    "train = corpus[:spl]\n",
    "test = corpus[spl:]\n",
    "\n",
    "# Remove rare words from the corpus\n",
    "fdist = nltk.FreqDist(w for w in train)\n",
    "vocabulary = set(map(lambda x: x[0], filter(lambda x: x[1] >= 5, fdist.items())))\n",
    "\n",
    "train = map(lambda x: x if x in vocabulary else \"*unknown*\", train)\n",
    "test = map(lambda x: x if x in vocabulary else \"*unknown*\", test)\n",
    "\n",
    "print(\"... train\")\n",
    "# from nltk.model import NgramModel\n",
    "# from nltk.probability import LidstoneProbDist\n",
    "\n",
    "print(train.sents)\n",
    "estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2) \n",
    "lm = NgramModel(5, train, estimator=estimator)\n",
    "# model = MLE(5)\n",
    "# model.fit(train, )\n",
    "\n",
    "print (\"len(corpus) = %s, len(vocabulary) = %s, len(train) = %s, len(test) = %s\" % ( len(corpus), len(vocabulary), len(train), len(test) ))\n",
    "print (\"perplexity(test) =\", lm.perplexity(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset here\n",
    "\n",
    "# Generate matrix for RBM\n",
    "\n",
    "# Convert dataset into a matrix then FloatTensor\n",
    "# Create a document-word matrix\n",
    "docs = list(range(len(documents)))\n",
    "words = []\n",
    "for token in tokenized:\n",
    "    for word in token:\n",
    "        if word not in words:\n",
    "            words.append(token)\n",
    "doc_word = np.zeros((len(docs), len(words)), dtype=int)\n",
    "print(doc_word.shape)\n",
    "row, col = doc_word.shape\n",
    "for i in range(row):\n",
    "    for j in range(col):\n",
    "        if words[j] in tokenized[i]:\n",
    "            doc_word[i][j] = 1\n",
    "            \n",
    "training_set = doc_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset here\n",
    "\n",
    "# # Generate matrix for RBM\n",
    "\n",
    "# # Convert dataset into a matrix then FloatTensor\n",
    "# # Create a document-word matrix\n",
    "# training_set = np.load('hawthorne.dat')\n",
    "# training_set = training_set[:5]\n",
    "# # Remove all columns of 0s\n",
    "\n",
    "# filt = np.where(training_set.sum(axis=0)==0)\n",
    "# training_set = np.delete(training_set, filt, 1)\n",
    "\n",
    "# print((training_set).shape)\n",
    "# training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBM Architecture\n",
    "class RBM():\n",
    "    def __init__(self, nv, nh):\n",
    "        self.W = torch.randn(nh, nv)\n",
    "        self.a = torch.randn(1, nh)\n",
    "        self.b = torch.randn(1, nv)\n",
    "\n",
    "    def sample_h(self, x):\n",
    "        wx = torch.mm(x, self.W.t())\n",
    "        activation = wx + self.a.expand_as(wx)\n",
    "        p_h_given_v = torch.sigmoid(activation)\n",
    "        return p_h_given_v, torch.bernoulli(p_h_given_v)\n",
    "\n",
    "    def sample_v(self, y):\n",
    "        wy = torch.mm(y, self.W)\n",
    "        activation = wy + self.b.expand_as(wy)\n",
    "        p_v_given_h = torch.sigmoid(activation)\n",
    "        return p_v_given_h, torch.bernoulli(p_v_given_h)\n",
    "\n",
    "    def train(self, v0, vk, ph0, phk):\n",
    "        self.W += torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)\n",
    "        self.b += torch.sum((v0 - vk), 0)\n",
    "        self.a += torch.sum((ph0 - phk), 0)\n",
    "        \n",
    "    def perplexity(self, R, vk, hk):\n",
    "        R = R.numpy()\n",
    "        vk = vk.numpy()\n",
    "        hk = hk.numpy()\n",
    "        w = self.W.numpy()\n",
    "        b = self.b.numpy()\n",
    "        \n",
    "        return np.exp((hk.T*w.T + b.T)*R.T*vk + b.T*vk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of visible node\n",
    "nv = len(training_set[0])\n",
    "# Number of hidden node\n",
    "nh = 7451\n",
    "# Batch size\n",
    "batch_size = 2\n",
    "# Initialize the model\n",
    "rbm = RBM(nv, nh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "nb_users = len(tokenized)\n",
    "tokenized = torch.FloatTensor(training_set)\n",
    "print (nb_users)\n",
    "nb_epoch = 7\n",
    "x = []\n",
    "y_loss = []\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(0, nb_users-batch_size, batch_size):\n",
    "        vk = tokenized[id_user:id_user+batch_size]\n",
    "        v0 = tokenized[id_user:id_user+batch_size]\n",
    "        ph0,_ = rbm.sample_h(v0)\n",
    "        for k in range(10):\n",
    "            _,hk = rbm.sample_h(vk)\n",
    "            _,vk = rbm.sample_v(hk)\n",
    "            vk[v0<0] = v0[v0<0]\n",
    "        phk,_ = rbm.sample_h(vk)\n",
    "        rbm.train(v0, vk, ph0, phk)\n",
    "        train_loss += torch.mean(torch.abs(v0[v0>=0] - vk[v0>=0]))\n",
    "        s += 1.\n",
    "        print(rbm.perplexity(tokenized, vk, hk))\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
    "    x.append(epoch)\n",
    "    y_loss.append(train_loss/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.title('Loss function over epochs')\n",
    "plt.ylabel('L1 Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(x, y_loss)\n",
    "plt.show()\n",
    "plt.savefig('rbm_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = books[-1]\n",
    "file = os.path.join(filepath, book)\n",
    "with open(file) as f:\n",
    "    data = f.read()\n",
    "    data = data.replace(\"[^a-zA-Z#]\", \"\")\n",
    "    data = data.lower()\n",
    "\n",
    "test_tokenized = []\n",
    "# Word tokenization\n",
    "test_tokenized.append(word_tokenize(data))\n",
    "\n",
    "row, col = training_set.shape\n",
    "test_set = np.zeros((1, col), dtype=int)\n",
    "\n",
    "row, col = test_set.shape\n",
    "for i in range(row):\n",
    "    for j in range(col):\n",
    "        if words[j] in test_tokenized[i]:\n",
    "            test_tokenized[i][j] = 1\n",
    "test_tokenized = torch.FloatTensor(test_set)\n",
    "vk = tokenized[:batch_size]\n",
    "v0 = tokenized[:batch_size]\n",
    "ph0,_ = rbm.sample_h(v0)\n",
    "\n",
    "for k in range(10):\n",
    "    _,hk = rbm.sample_h(vk)\n",
    "    _,vk = rbm.sample_v(hk)\n",
    "    vk[v0<0] = v0[v0<0]\n",
    "rbm.perplexity(test_tokenized, vk, hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Bilinear Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at lplmodel.py file instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
